{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms_func = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_datasets = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets\\\\Seen Datasets/train',\n",
    "    transform=transforms_func\n",
    ")\n",
    "val_datasets = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets\\\\Seen Datasets/val',\n",
    "    transform=transforms_func\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data_dir = 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets\\\\Seen Datasets'\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_datasets, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get a single image and label from the training dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m img, lbl \u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# Index can be any valid integer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the image from (C, H, W) to (H, W, C)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get a single image and label from the training dataset\n",
    "img, lbl =train_dataloader[0] # Index can be any valid integer\n",
    "\n",
    "# Convert the image from (C, H, W) to (H, W, C)\n",
    "img = img.permute(1, 2, 0)\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(f'Label: {lbl}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.features_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512 * 13 * 13, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 25)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, \n",
    "          criteria,  # Use CrossEntropyLoss for multi-class classification\n",
    "          optimizer, \n",
    "          train_dataloader, \n",
    "          val_dataloader, \n",
    "          epochs, \n",
    "          device, \n",
    "          val_iteration=1):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ## Train \n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_correct = 0\n",
    "        train_step = 0\n",
    "        for batch_images, batch_labels in tqdm(train_dataloader):\n",
    "            batch_images = batch_images.to(device ,non_blocking=True)\n",
    "            batch_labels = batch_labels.to(device ,non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            results = model(batch_images)\n",
    "            loss = criteria(results, batch_labels)  # CrossEntropyLoss expects raw logits\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()  # calculate gradients\n",
    "            optimizer.step()  # update weights using gradients\n",
    "\n",
    "            train_step += 1\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # Training accuracy calculation\n",
    "            _, predicted = torch.max(results, 1)  # Get the index of the max log-probability\n",
    "            epoch_train_correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        epoch_train_loss /= train_step\n",
    "        epoch_train_accuracy = epoch_train_correct / len(train_dataloader.dataset)\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        train_accuracy.append(epoch_train_accuracy)\n",
    "        print(f\"Epoch {epoch} of {epochs}: train loss = {epoch_train_loss:.4f}, train accuracy = {epoch_train_accuracy:.4f}\")\n",
    "\n",
    "        ## Validation \n",
    "        if epoch % val_iteration == 0:\n",
    "            model.eval()\n",
    "\n",
    "            epoch_val_loss = 0\n",
    "            epoch_val_correct = 0\n",
    "            val_step = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_images, batch_labels in val_dataloader:\n",
    "                    batch_images = batch_images.to(device ,non_blocking=True)\n",
    "                    batch_labels = batch_labels.to(device ,non_blocking=True)\n",
    "                    \n",
    "                    # forward pass\n",
    "                    results = model(batch_images)\n",
    "                    loss = criteria(results, batch_labels)\n",
    "                    epoch_val_loss += loss.item()\n",
    "                    val_step += 1\n",
    "\n",
    "                    # Validation accuracy calculation\n",
    "                    _, predicted = torch.max(results, 1)  # Get the index of the max log-probability\n",
    "                    epoch_val_correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "            epoch_val_loss /= val_step\n",
    "            epoch_val_accuracy = epoch_val_correct / len(val_dataloader.dataset)\n",
    "            val_loss.append(epoch_val_loss)\n",
    "            val_accuracy.append(epoch_val_accuracy)\n",
    "            print(f\"Epoch {epoch} of {epochs}: validation loss = {epoch_val_loss:.4f}, validation accuracy = {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "    return train_loss, val_loss, train_accuracy, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 \n",
    "\n",
    "model = CustomCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define loss function and optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/704 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [53:32<00:00,  4.56s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10: train loss = 2.5663, train accuracy = 0.1556\n",
      "Epoch 1 of 10: validation loss = 2.2890, validation accuracy = 0.2289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/704 [01:24<56:46,  4.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss, val_loss, train_accuracy, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criteria, optimizer, train_dataloader, val_dataloader, epochs, device, val_iteration)\u001b[0m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# update weights using gradients\u001b[39;00m\n\u001b[0;32m     36\u001b[0m train_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 37\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Training accuracy calculation\u001b[39;00m\n\u001b[0;32m     40\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(results, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get the index of the max log-probability\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, train_accuracy, val_accuracy = train(\n",
    "    model,\n",
    "    criteria=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 108/704 [08:22<46:13,  4.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 133\u001b[0m\n\u001b[0;32m    130\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    132\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomCNN()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 133\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 99\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs, device, checkpoint_path)\u001b[0m\n\u001b[0;32m     97\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 99\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    101\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Transformations\n",
    "transforms_func = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_datasets = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets\\\\Seen Datasets/train',\n",
    "    transform=transforms_func\n",
    ")\n",
    "val_datasets = datasets.ImageFolder(\n",
    "    'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets\\\\Seen Datasets/val',\n",
    "    transform=transforms_func\n",
    ")\n",
    "\n",
    "# Dataloaders with increased num_workers\n",
    "data_dir = 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets\\\\Seen Datasets'\n",
    "batch_size = 32 # Increased batch size for faster training\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_datasets, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define a custom CNN model\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.features_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512 * 13 * 13, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 25)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Function to save checkpoints\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "# Training function with checkpoint saving\n",
    "def train(model, train_loader, val_loader, epochs, device, checkpoint_path='checkpoint.pth.tar'):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}, resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device ,non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                corrects += (preds == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = corrects / total\n",
    "        print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 10\n",
    "\n",
    "model = CustomCNN().to(device)\n",
    "train(model, train_dataloader, val_dataloader, epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Images have been resized and replaced.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "directory = r'C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train'\n",
    "\n",
    "\n",
    "    # List to store folder paths\n",
    "folder_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for dir_name in dirs:\n",
    "            # Construct the full path of the folder\n",
    "        folder_path = os.path.join(root, dir_name)\n",
    "        folder_paths.append(folder_path)\n",
    "\n",
    "\n",
    "\n",
    "# Resize images\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            # Resize image\n",
    "            img_resized = img.resize((416, 416))\n",
    "            # Save resized image, overwriting the original file\n",
    "            img_resized.save(img_path)\n",
    "            print('1')\n",
    "\n",
    "print(\"Images have been resized and replaced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Asian-Green-Bee-Eater\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Brown-Headed-Barbet\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Cattle-Egret\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Common-Kingfisher\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Common-Myna\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Common-Rosefinch\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Common-Tailorbird\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Coppersmith-Barbet\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Forest-Wagtail\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Gray-Wagtail\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Hoopoe\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\House-Crow\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Indian-Grey-Hornbill\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Indian-Peacock\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Indian-Pitta\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Indian-Roller\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Jungle-Babbler\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Northern-Lapwing\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Red-Wattled-Lapwing\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Ruddy-Shelduck\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Rufous-Treepie\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\Sarus-Crane\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\White-Breasted-Kingfisher\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\White-Breasted-Waterhen\n",
      "C:\\Users\\sshak\\Downloads\\Seen Datasets1\\Seen Datasets\\train\\White-Wagtail\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Asian-Green-Bee-Eater',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Brown-Headed-Barbet',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Cattle-Egret',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Common-Kingfisher',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Common-Myna',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Common-Rosefinch',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Common-Tailorbird',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Coppersmith-Barbet',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Forest-Wagtail',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Gray-Wagtail',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Hoopoe',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\House-Crow',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Indian-Grey-Hornbill',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Indian-Peacock',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Indian-Pitta',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Indian-Roller',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Jungle-Babbler',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Northern-Lapwing',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Red-Wattled-Lapwing',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Ruddy-Shelduck',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Rufous-Treepie',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\Sarus-Crane',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\White-Breasted-Kingfisher',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\White-Breasted-Waterhen',\n",
       " 'C:\\\\Users\\\\sshak\\\\Downloads\\\\Seen Datasets1\\\\Seen Datasets\\\\train\\\\White-Wagtail']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Path to your main directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset replacement completed.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  \n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor()  # Convert image to tensor\n",
    "    # Note: Normalize is not included here as it would alter the pixel values\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset_path = r'C:\\Users\\sshak\\Downloads\\Seen Datasets\\Seen Datasets'\n",
    "train_dataset = datasets.ImageFolder(os.path.join(dataset_path, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dataset_path, 'val'), transform=transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Function to save images\n",
    "def save_augmented_images(dataloader, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        for j, (image, label) in enumerate(zip(images, labels)):\n",
    "            img = transforms.ToPILImage()(image)  # Convert tensor back to PIL Image\n",
    "            class_dir = os.path.join(save_dir, str(label.item()))\n",
    "            if not os.path.exists(class_dir):\n",
    "                os.makedirs(class_dir)\n",
    "            img.save(os.path.join(class_dir, f'augmented_{i}_{j}.jpg'))  # Save as JPG\n",
    "\n",
    "# Save the augmented training images\n",
    "train_save_dir = os.path.join(dataset_path, 'train_augmented')\n",
    "save_augmented_images(train_dataloader, train_save_dir)\n",
    "\n",
    "# Save the augmented validation images\n",
    "val_save_dir = os.path.join(dataset_path, 'val_augmented')\n",
    "save_augmented_images(val_dataloader, val_save_dir)\n",
    "\n",
    "# Replace old dataset directories with new ones\n",
    "# Replace training set\n",
    "shutil.rmtree(os.path.join(dataset_path, 'train'))\n",
    "shutil.move(train_save_dir, os.path.join(dataset_path, 'train'))\n",
    "\n",
    "# Replace validation set\n",
    "shutil.rmtree(os.path.join(dataset_path, 'val'))\n",
    "shutil.move(val_save_dir, os.path.join(dataset_path, 'val'))\n",
    "\n",
    "print(\"Dataset replacement completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
